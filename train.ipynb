{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搭建vit模型，相关代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import math\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
    "from torch.nn.modules.utils import _pair\n",
    "from scipy import ndimage\n",
    "\n",
    "import models.configs as configs\n",
    "\n",
    "from .modeling_resnet import ResNetV2\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\" \n",
    "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
    "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
    "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
    "FC_0 = \"MlpBlock_3/Dense_0\"\n",
    "FC_1 = \"MlpBlock_3/Dense_1\"\n",
    "ATTENTION_NORM = \"LayerNorm_0\"\n",
    "MLP_NORM = \"LayerNorm_2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy转torch\n",
    "def np2th(weights, conv=False):\n",
    "    # 如果conv，HWIO -> OIHW\n",
    "    if conv:\n",
    "        weights = weights.transpose([3, 2, 0, 1])\n",
    "    return torch.from_numpy(weights)\n",
    "\n",
    "# 激活函数swish\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "# 激活函数字典\n",
    "ACT2FN = {\n",
    "    \"gelu\": torch.nn.functional.gelu,\n",
    "    \"relu\": torch.nn.functional.relu,\n",
    "    \"swish\": swish,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过模型原理介绍可知，VIT模型以transformer为基础，因此需要先搭建ffn，注意力机制等组件，再将其与图像预处理，编码嵌入层等拼接起来得到一个完整的vit模型\n",
    "\n",
    "对于图像编码，本例中为VIT-B_16\n",
    "卷积核大小16*16，步长为16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings构建patch embedding和position embedding\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, config, img_size, in_channels=3):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.hybrid = None\n",
    "        img_size = _pair(img_size)\n",
    "\n",
    "        # patch_size大小和n_patches个数\n",
    "        if config.patches.get(\"grid\") is not None:\n",
    "            grid_size = config.patches[\"grid\"]\n",
    "            patch_size = (\n",
    "                img_size[0] // 16 // grid_size[0],\n",
    "                img_size[1] // 16 // grid_size[1],\n",
    "            )\n",
    "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
    "            self.hybrid = True\n",
    "        else:\n",
    "            patch_size = _pair(config.patches[\"size\"])\n",
    "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "            self.hybrid = False\n",
    "\n",
    "        #混合模型\n",
    "        if self.hybrid:\n",
    "            self.hybrid_model = ResNetV2(\n",
    "                block_units=config.resnet.num_layers,\n",
    "                width_factor=config.resnet.width_factor,\n",
    "            )\n",
    "            in_channels = self.hybrid_model.width * 16\n",
    "        # 初始化patch embedding\n",
    "        self.patch_embeddings = Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=config.hidden_size,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size,\n",
    "        )\n",
    "        # 初始化位置编码position_embeddings\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, n_patches + 1, config.hidden_size)\n",
    "        )\n",
    "        # 第0个位置patch，表示分类特征\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"]) # dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        # 拓展cls_token维度，16*1*768\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "\n",
    "        if self.hybrid:\n",
    "            x = self.hybrid_model(x)\n",
    "        # patch embedding，16*768*14*14\n",
    "        x = self.patch_embeddings(x)\n",
    "        x = x.flatten(2)\n",
    "        # 变换维度，16*196*768\n",
    "        x = x.transpose(-1, -2)\n",
    "        # 拼接cls_token，加入分类特征\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # 加入位置编码\n",
    "        embeddings = x + self.position_embeddings\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用多头注意力机制，对于采用的ViT-B_16，为12个\n",
    "# 首先构建q,k,v三个辅助向量，将q,k,v维度(16, 197, 768)转换成(16, 12, 197, 64)\n",
    "# 然后获得q,k的相似性qk，因为获得的是两两之间的关系，所以维度为(16, 12, 197, 197)\n",
    "# 经过softmax后，消除量纲，得到提取到的特征向量qkv，维度为(16, 12, 197, 64)，再还原成(16, 197, 768)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Attention, self).__init__()\n",
    "        self.vis = vis #是否可视化\n",
    "\n",
    "        self.num_attention_heads = config.transformer[\"num_heads\"] # 多头注意力机制的头数\n",
    "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads) # 每个头的维度\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size # q，k，a的总维度\n",
    "\n",
    "        # 定义Wq，Wk，Wv，通过全连接网络生成\n",
    "        self.query = Linear(config.hidden_size, self.all_head_size) \n",
    "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.out = Linear(config.hidden_size, config.hidden_size) # 定义W0，通过全连接网络生成\n",
    "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"]) # dropout\n",
    "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"]) # dropout\n",
    "\n",
    "        self.softmax = Softmax(dim=-1)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        # 将q，k，v的维度变换为(batch_size, num_attention_heads, seq_length, attention_head_size)\n",
    "        # 16，197，768 -> 16，197，12，64\n",
    "        new_x_shape = x.size()[:-1] + (\n",
    "            self.num_attention_heads,\n",
    "            self.attention_head_size,\n",
    "        )\n",
    "        x = x.view(*new_x_shape)\n",
    "        # 16，197，12，64 -> 16，12，197，64\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # 构建q，k，v，维度为16, 197, 768\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        # 将q，k，v的维度变换为(batch_size, num_attention_heads, seq_length, attention_head_size)\n",
    "        # 16，197，768 -> 16，12，197，64\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # 获得q，k的相似性，维度为16，12，197，197\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = self.softmax(attention_scores)\n",
    "        weights = attention_probs if self.vis else None\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        # 16，12，197，64 -> 16，197，768\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        attention_output = self.out(context_layer)\n",
    "        attention_output = self.proj_dropout(attention_output)\n",
    "        return attention_output, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP用于分类的层结构，由全连接+GELU激活函数+Dropout组成\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(Mlp, self).__init__()\n",
    "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
    "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
    "        self.act_fn = ACT2FN[\"gelu\"]\n",
    "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer的Block结构，由Attention，MLP，LayerNorm组成\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Block, self).__init__()\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        self.ffn = Mlp(config)\n",
    "        self.attn = Attention(config, vis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        # 层归一化\n",
    "        x = self.attention_norm(x)\n",
    "        # 多头注意力机制\n",
    "        x, weights = self.attn(x)\n",
    "        # 残差连接\n",
    "        x = x + h\n",
    "\n",
    "        h = x\n",
    "        # 层归一化\n",
    "        x = self.ffn_norm(x)\n",
    "        # MLP\n",
    "        x = self.ffn(x)\n",
    "        # 残差连接\n",
    "        x = x + h\n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer的Encoder结构，由多个Block组成\n",
    "# Block的数量由num_layers指定\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config, vis):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vis = vis\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
    "        for _ in range(config.transformer[\"num_layers\"]):\n",
    "            layer = Block(config, vis)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = []\n",
    "        for layer_block in self.layer:\n",
    "            hidden_states, weights = layer_block(hidden_states)\n",
    "            if self.vis:\n",
    "                attn_weights.append(weights)\n",
    "        encoded = self.encoder_norm(hidden_states)\n",
    "        return encoded, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer，由Embedding和Encoder组成\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config, img_size, vis):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = Embeddings(config, img_size=img_size)\n",
    "        self.encoder = Encoder(config, vis)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoded, attn_weights = self.encoder(embedding_output)\n",
    "        return encoded, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VisionTransformer，由Transformer和Linear组成\n",
    "# 对于任意x，进行patch embedding和positional embedding后，输入Encoder中，经过L层的编码，取出第一个token的输出，输入到分类层中，得到分类结果\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, config, img_size=224, num_classes=21843, zero_head=False, vis=False\n",
    "    ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.zero_head = zero_head\n",
    "        self.classifier = config.classifier\n",
    "\n",
    "        self.transformer = Transformer(config, img_size, vis)\n",
    "        self.head = Linear(config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x, attn_weights = self.transformer(x)\n",
    "        logits = self.head(x[:, 0])\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits, attn_weights\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        with torch.no_grad():\n",
    "            if self.zero_head:\n",
    "                nn.init.zeros_(self.head.weight)\n",
    "                nn.init.zeros_(self.head.bias)\n",
    "            else:\n",
    "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
    "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
    "\n",
    "            self.transformer.embeddings.patch_embeddings.weight.copy_(\n",
    "                np2th(weights[\"embedding/kernel\"], conv=True)\n",
    "            )\n",
    "            self.transformer.embeddings.patch_embeddings.bias.copy_(\n",
    "                np2th(weights[\"embedding/bias\"])\n",
    "            )\n",
    "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
    "            self.transformer.encoder.encoder_norm.weight.copy_(\n",
    "                np2th(weights[\"Transformer/encoder_norm/scale\"])\n",
    "            )\n",
    "            self.transformer.encoder.encoder_norm.bias.copy_(\n",
    "                np2th(weights[\"Transformer/encoder_norm/bias\"])\n",
    "            )\n",
    "\n",
    "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
    "            posemb_new = self.transformer.embeddings.position_embeddings\n",
    "            if posemb.size() == posemb_new.size():\n",
    "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"load_pretrained: resized variant: %s to %s\"\n",
    "                    % (posemb.size(), posemb_new.size())\n",
    "                )\n",
    "                ntok_new = posemb_new.size(1)\n",
    "\n",
    "                if self.classifier == \"token\":\n",
    "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "                    ntok_new -= 1\n",
    "                else:\n",
    "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "\n",
    "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
    "                gs_new = int(np.sqrt(ntok_new))\n",
    "                print(\"load_pretrained: grid-size from %s to %s\" % (gs_old, gs_new))\n",
    "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
    "\n",
    "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
    "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
    "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
    "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
    "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
    "\n",
    "            for bname, block in self.transformer.encoder.named_children():\n",
    "                for uname, unit in block.named_children():\n",
    "                    unit.load_from(weights, n_block=uname)\n",
    "\n",
    "            if self.transformer.embeddings.hybrid:\n",
    "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(\n",
    "                    np2th(weights[\"conv_root/kernel\"], conv=True)\n",
    "                )\n",
    "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
    "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
    "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
    "\n",
    "                for (\n",
    "                    bname,\n",
    "                    block,\n",
    "                ) in self.transformer.embeddings.hybrid_model.body.named_children():\n",
    "                    for uname, unit in block.named_children():\n",
    "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置文件\n",
    "CONFIGS = {\n",
    "    \"ViT-B_16\": configs.get_b16_config(),\n",
    "    \"ViT-B_32\": configs.get_b32_config(),\n",
    "    \"ViT-L_16\": configs.get_l16_config(),\n",
    "    \"ViT-L_32\": configs.get_l32_config(),\n",
    "    \"ViT-H_14\": configs.get_h14_config(),\n",
    "    \"R50-ViT-B_16\": configs.get_r50_b16_config(),\n",
    "    \"testing\": configs.get_testing(),\n",
    "}\n",
    "# config属性\n",
    "# patches：patch的大小\n",
    "# hidden_size：隐藏层大小\n",
    "# transformer：transformer的配置\n",
    "# transformer.mlp_dim：mlp的大小\n",
    "# transformer.num_heads：头的数量\n",
    "# transformer.num_layers：层数\n",
    "# transformer.attention_dropout_rate：attention的dropout\n",
    "# transformer.dropout_rate：dropout\n",
    "# classifier：分类器\n",
    "# representation_size：representation的大小\n",
    "#\n",
    "# vis：可视化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc93666e76b018e5c14b3227280c49f3e9c09e02ce547d1fb0570213849fb58f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
